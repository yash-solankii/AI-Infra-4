{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad46fcce-35f6-47f3-b708-0270a4a6c55a",
   "metadata": {},
   "source": [
    "### AIDI-1006 Assignment. done by Yash solanki (200579052) and Priyanka priyanka (200585516)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd9a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "896ea7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>234</td>\n",
       "      <td>235</td>\n",
       "      <td>239</td>\n",
       "      <td>Seeking serenity in the melody of raindrops, ...</td>\n",
       "      <td>Serenity</td>\n",
       "      <td>2020-04-12 19:30:00</td>\n",
       "      <td>RaindropHarmony</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Serenity #RaindropMelody</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>seeking serenity melody raindrop tranquil esca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>337</td>\n",
       "      <td>338</td>\n",
       "      <td>342</td>\n",
       "      <td>Overwhelmed by the support received during a p...</td>\n",
       "      <td>Overwhelmed</td>\n",
       "      <td>2021-05-20 17:30:00</td>\n",
       "      <td>PersonalJourney</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Overwhelmed #SupportiveCommunity</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>overwhelmed support received personal challenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>229</td>\n",
       "      <td>230</td>\n",
       "      <td>234</td>\n",
       "      <td>Melancholy painting the world in hues of nost...</td>\n",
       "      <td>Melancholy</td>\n",
       "      <td>2020-01-05 20:00:00</td>\n",
       "      <td>CanvasDreamer</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Melancholy #BittersweetMemories</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>melancholy painting world hue nostalgia canvas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>610</td>\n",
       "      <td>611</td>\n",
       "      <td>615</td>\n",
       "      <td>Embarked on a road trip to revisit cherished p...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>2023-06-19 16:30:00</td>\n",
       "      <td>RoadTripSenior</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#CherishedPlaces #SeniorTravel</td>\n",
       "      <td>22.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>embarked road trip revisit cherished place pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>540</td>\n",
       "      <td>541</td>\n",
       "      <td>545</td>\n",
       "      <td>Celebrating a historic victory in the World Cu...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>2018-07-15 21:30:00</td>\n",
       "      <td>FootballFanWorldCupCelebration</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Joy #WorldCupTriumph</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>celebrating historic victory world cup nation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>221</td>\n",
       "      <td>222</td>\n",
       "      <td>224</td>\n",
       "      <td>Flipping through the pages of an old yearbook...</td>\n",
       "      <td>Nostalgia</td>\n",
       "      <td>2017-06-18 14:45:00</td>\n",
       "      <td>YearbookExplorer</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Nostalgia #YearbookMemories</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>India</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>flipping page old yearbook nostalgia painting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>226</td>\n",
       "      <td>227</td>\n",
       "      <td>231</td>\n",
       "      <td>Indifferent to the noise of the world, a sile...</td>\n",
       "      <td>Indifference</td>\n",
       "      <td>2023-03-10 16:30:00</td>\n",
       "      <td>SilentSpectator</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Indifference #SilentObserver</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>India</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>indifferent noise world silent observer midst ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>451</td>\n",
       "      <td>452</td>\n",
       "      <td>456</td>\n",
       "      <td>Haunted by the specter of lost possibilities, ...</td>\n",
       "      <td>Regret</td>\n",
       "      <td>2019-11-08 14:45:00</td>\n",
       "      <td>PossibilityGhost</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Regret #FadingPossibilities</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Spain</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>haunted specter lost possibility ghost refuse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>632</td>\n",
       "      <td>633</td>\n",
       "      <td>637</td>\n",
       "      <td>Hosted a photography exhibition featuring snap...</td>\n",
       "      <td>Gratitude</td>\n",
       "      <td>2023-07-11 18:45:00</td>\n",
       "      <td>SeniorPhotographerExhibitor</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#LifeInPictures #SeniorExhibition</td>\n",
       "      <td>22.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>hosted photography exhibition featuring snapsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "      <td>Amused by the antics of my petâ€”it's pure amus...</td>\n",
       "      <td>Amusement</td>\n",
       "      <td>2023-02-23 14:20:00</td>\n",
       "      <td>PetAmuser</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Amusement #PetAntics</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>amused antic pet pure amusement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "234           234           235         239   \n",
       "337           337           338         342   \n",
       "229           229           230         234   \n",
       "610           610           611         615   \n",
       "540           540           541         545   \n",
       "221           221           222         224   \n",
       "226           226           227         231   \n",
       "451           451           452         456   \n",
       "632           632           633         637   \n",
       "83             83            84          86   \n",
       "\n",
       "                                                  Text          Sentiment  \\\n",
       "234   Seeking serenity in the melody of raindrops, ...   Serenity           \n",
       "337  Overwhelmed by the support received during a p...     Overwhelmed      \n",
       "229   Melancholy painting the world in hues of nost...   Melancholy         \n",
       "610  Embarked on a road trip to revisit cherished p...               Joy    \n",
       "540  Celebrating a historic victory in the World Cu...               Joy    \n",
       "221   Flipping through the pages of an old yearbook...   Nostalgia          \n",
       "226   Indifferent to the noise of the world, a sile...   Indifference       \n",
       "451  Haunted by the specter of lost possibilities, ...            Regret    \n",
       "632  Hosted a photography exhibition featuring snap...         Gratitude    \n",
       "83    Amused by the antics of my petâ€”it's pure amus...      Amusement       \n",
       "\n",
       "               Timestamp                              User     Platform  \\\n",
       "234  2020-04-12 19:30:00                RaindropHarmony       Twitter     \n",
       "337  2021-05-20 17:30:00                  PersonalJourney     Facebook    \n",
       "229  2020-01-05 20:00:00                CanvasDreamer        Instagram    \n",
       "610  2023-06-19 16:30:00                   RoadTripSenior     Facebook    \n",
       "540  2018-07-15 21:30:00   FootballFanWorldCupCelebration    Instagram    \n",
       "221  2017-06-18 14:45:00                YearbookExplorer     Instagram    \n",
       "226  2023-03-10 16:30:00                SilentSpectator      Instagram    \n",
       "451  2019-11-08 14:45:00                 PossibilityGhost      Twitter    \n",
       "632  2023-07-11 18:45:00      SeniorPhotographerExhibitor      Twitter    \n",
       "83   2023-02-23 14:20:00                  PetAmuser          Instagram    \n",
       "\n",
       "                                          Hashtags  Retweets  Likes  \\\n",
       "234      #Serenity #RaindropMelody                      25.0   50.0   \n",
       "337    #Overwhelmed #SupportiveCommunity                25.0   50.0   \n",
       "229      #Melancholy #BittersweetMemories               13.0   26.0   \n",
       "610                #CherishedPlaces #SeniorTravel       22.0   45.0   \n",
       "540                         #Joy #WorldCupTriumph       40.0   80.0   \n",
       "221       #Nostalgia #YearbookMemories                  12.0   25.0   \n",
       "226      #Indifference #SilentObserver                   9.0   18.0   \n",
       "451                  #Regret #FadingPossibilities       18.0   35.0   \n",
       "632             #LifeInPictures #SeniorExhibition       22.0   45.0   \n",
       "83    #Amusement #PetAntics                             18.0   35.0   \n",
       "\n",
       "                 Country  Year  Month  Day  Hour  \\\n",
       "234       Canada          2020      4   12    19   \n",
       "337   USA                 2021      5   20    17   \n",
       "229        USA            2020      1    5    20   \n",
       "610               Canada  2023      6   19    16   \n",
       "540              Brazil   2018      7   15    21   \n",
       "221       India           2017      6   18    14   \n",
       "226       India           2023      3   10    16   \n",
       "451               Spain   2019     11    8    14   \n",
       "632               Canada  2023      7   11    18   \n",
       "83         UK             2023      2   23    14   \n",
       "\n",
       "                                          cleaned_text  \n",
       "234  seeking serenity melody raindrop tranquil esca...  \n",
       "337    overwhelmed support received personal challenge  \n",
       "229  melancholy painting world hue nostalgia canvas...  \n",
       "610  embarked road trip revisit cherished place pas...  \n",
       "540  celebrating historic victory world cup nation ...  \n",
       "221  flipping page old yearbook nostalgia painting ...  \n",
       "226  indifferent noise world silent observer midst ...  \n",
       "451  haunted specter lost possibility ghost refuse ...  \n",
       "632  hosted photography exhibition featuring snapsh...  \n",
       "83                     amused antic pet pure amusement  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sentimentdataset.csv')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7344407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0   Enjoying a beautiful day at the park!        ...   \n",
      "1   Traffic was terrible this morning.           ...   \n",
      "2   Just finished an amazing workout! ðŸ’ª          ...   \n",
      "3   Excited about the upcoming weekend getaway!  ...   \n",
      "4   Trying out a new recipe for dinner tonight.  ...   \n",
      "\n",
      "                       cleaned_text  \n",
      "0       enjoying beautiful day park  \n",
      "1          traffic terrible morning  \n",
      "2          finished amazing workout  \n",
      "3  excited upcoming weekend getaway  \n",
      "4  trying new recipe dinner tonight  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('sentimentdataset.csv')\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column\n",
    "df['cleaned_text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Print the first few rows of the DataFrame to verify the results\n",
    "print(df[['Text', 'cleaned_text']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389e5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of TF-IDF Features: (732, 2154)\n",
      "\n",
      "First Few Rows of TF-IDF Features:\n",
      "   ablaze  abstract  abyss  academic  acceptance  accepts  accidentally  \\\n",
      "0     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "1     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "2     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "3     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "4     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "\n",
      "   accomplished  accomplishing  accomplishment  ...  year  yearbook  yearning  \\\n",
      "0           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "1           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "2           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "3           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "4           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "\n",
      "   yet  york  young  zen  zero  zest  zestful  \n",
      "0  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "1  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "2  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "3  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "4  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "\n",
      "[5 rows x 2154 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the cleaned text data into TF-IDF features\n",
    "X = vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
    "\n",
    "# Display the shape of the TF-IDF features\n",
    "print(\"\\nShape of TF-IDF Features:\", X.shape)\n",
    "\n",
    "# Display the first few rows of the TF-IDF features\n",
    "print(\"\\nFirst Few Rows of TF-IDF Features:\")\n",
    "print(pd.DataFrame(X, columns=vectorizer.get_feature_names_out()).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32862466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Few Labels:\n",
      "0     Positive  \n",
      "1     Negative  \n",
      "2     Positive  \n",
      "3     Positive  \n",
      "4     Neutral   \n",
      "Name: Sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extract labels (sentiment) from the DataFrame\n",
    "y = df['Sentiment']  # Assuming the sentiment labels are in the 'Sentiment' column\n",
    "\n",
    "# Display the first few labels\n",
    "print(\"\\nFirst Few Labels:\")\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89f9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad980d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (585, 2154)\n",
      "X_test shape: (147, 2154)\n",
      "y_train shape: (585,)\n",
      "y_test shape: (147,)\n"
     ]
    }
   ],
   "source": [
    "# Display the shapes of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1e53f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Parameters:\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "# Display the model's parameters\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(model.get_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77185128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13ff0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment labels for the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b50012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11564625850340136\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         Acceptance          0.00      0.00      0.00         2\n",
      "           Admiration        0.00      0.00      0.00         1\n",
      "        Admiration           0.00      0.00      0.00         1\n",
      "         Affection           0.00      0.00      0.00         1\n",
      "      Ambivalence            0.00      0.00      0.00         1\n",
      "         Anger               0.00      0.00      0.00         1\n",
      "        Anticipation         0.00      0.00      0.00         1\n",
      "        Arousal              0.00      0.00      0.00         3\n",
      "                  Awe        0.00      0.00      0.00         1\n",
      "         Awe                 0.00      0.00      0.00         1\n",
      "                  Bad        0.00      0.00      0.00         1\n",
      "             Betrayal        0.00      0.00      0.00         2\n",
      "        Betrayal             0.00      0.00      0.00         1\n",
      "         Bitter              0.00      0.00      0.00         1\n",
      "           Bitterness        0.00      0.00      0.00         1\n",
      "          Bittersweet        0.00      0.00      0.00         1\n",
      "              Boredom        0.00      0.00      0.00         1\n",
      "         Calmness            0.00      0.00      0.00         1\n",
      "          Captivation        0.00      0.00      0.00         1\n",
      "     Celestial Wonder        0.00      0.00      0.00         1\n",
      "             Colorful        0.00      0.00      0.00         1\n",
      "      Confusion              0.00      0.00      0.00         3\n",
      "           Connection        0.00      0.00      0.00         1\n",
      "        Contemplation        0.00      0.00      0.00         1\n",
      "          Contentment        0.00      0.00      0.00         3\n",
      "        Contentment          0.00      0.00      0.00         1\n",
      "         Coziness            0.00      0.00      0.00         1\n",
      "         Creativity          0.00      0.00      0.00         1\n",
      "            Curiosity        0.00      0.00      0.00         2\n",
      "          Curiosity          0.00      0.00      0.00         1\n",
      "      Curiosity              0.00      0.00      0.00         2\n",
      "           Desolation        0.00      0.00      0.00         1\n",
      "           Devastated        0.00      0.00      0.00         2\n",
      "              Disgust        0.00      0.00      0.00         1\n",
      "         Disgust             0.00      0.00      0.00         2\n",
      "        Elation              0.00      0.00      0.00         3\n",
      "             Elegance        0.00      0.00      0.00         1\n",
      "          Embarrassed        0.00      0.00      0.00         1\n",
      "       EmotionalStorm        0.00      0.00      0.00         1\n",
      "        Empowerment          0.00      0.00      0.00         1\n",
      "         Enjoyment           0.00      0.00      0.00         2\n",
      "           Enthusiasm        0.00      0.00      0.00         1\n",
      "              Envious        0.00      0.00      0.00         2\n",
      "  Envisioning History        0.00      0.00      0.00         1\n",
      "         Euphoria            0.00      0.00      0.00         1\n",
      "           Excitement        0.12      0.67      0.20         3\n",
      "         Excitement          0.00      0.00      0.00         3\n",
      "        Excitement           0.00      0.00      0.00         1\n",
      "         Fear                0.00      0.00      0.00         1\n",
      "              Fearful        0.00      0.00      0.00         1\n",
      "           Frustrated        0.00      0.00      0.00         1\n",
      "          Frustration        0.00      0.00      0.00         3\n",
      "         Fulfillment         0.00      0.00      0.00         2\n",
      "             Grateful        0.00      0.00      0.00         1\n",
      "      Grief                  0.00      0.00      0.00         1\n",
      "                Happy        0.00      0.00      0.00         6\n",
      "                 Hate        0.00      0.00      0.00         2\n",
      "           Heartbreak        0.00      0.00      0.00         2\n",
      "              Hopeful        1.00      1.00      1.00         1\n",
      "        InnerJourney         0.00      0.00      0.00         1\n",
      "        Inspiration          0.00      0.00      0.00         1\n",
      "             Inspired        0.00      0.00      0.00         1\n",
      "            Isolation        0.00      0.00      0.00         1\n",
      "          Jealousy           0.00      0.00      0.00         1\n",
      "                  Joy        0.14      0.88      0.25         8\n",
      "         Joy                 0.00      0.00      0.00         1\n",
      "        JoyfulReunion        0.00      0.00      0.00         1\n",
      "         Kind                0.00      0.00      0.00         1\n",
      "           Loneliness        1.00      1.00      1.00         1\n",
      "      Loneliness             0.00      0.00      0.00         1\n",
      "             LostLove        0.00      0.00      0.00         1\n",
      "      Melancholy             0.00      0.00      0.00         2\n",
      "       Miscalculation        0.00      0.00      0.00         1\n",
      "              Neutral        0.00      0.00      0.00         1\n",
      "        Nostalgia            0.00      0.00      0.00         1\n",
      "      Nostalgia              0.00      0.00      0.00         1\n",
      "      Numbness               0.00      0.00      0.00         1\n",
      "          Overwhelmed        0.00      0.00      0.00         1\n",
      "              Playful        0.00      0.00      0.00         2\n",
      "            Positive         0.08      0.67      0.14         9\n",
      "                Proud        0.00      0.00      0.00         1\n",
      "        Reflection           0.00      0.00      0.00         1\n",
      "       Regret                0.00      0.00      0.00         1\n",
      "           Resilience        0.00      0.00      0.00         1\n",
      "            Reverence        0.00      0.00      0.00         1\n",
      "         Sadness             0.00      0.00      0.00         2\n",
      "        Satisfaction         0.00      0.00      0.00         1\n",
      "             Serenity        0.00      0.00      0.00         2\n",
      "      Serenity               0.00      0.00      0.00         2\n",
      "             Solitude        0.00      0.00      0.00         1\n",
      "          Sorrow             0.00      0.00      0.00         1\n",
      "         Spark               0.00      0.00      0.00         1\n",
      "         Surprise            0.00      0.00      0.00         1\n",
      "        Thrill               0.00      0.00      0.00         1\n",
      "             Vibrancy        0.00      0.00      0.00         1\n",
      " Whispers of the Past        0.00      0.00      0.00         1\n",
      "                 Zest        0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.12       147\n",
      "             macro avg       0.02      0.04      0.03       147\n",
      "          weighted avg       0.03      0.12      0.04       147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30fb272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a CSV file\n",
    "df.to_csv('cleaned_sentiment_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f40930a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to an SQLite database\n",
    "conn = sqlite3.connect('sentiment_analysis.db')\n",
    "df.to_sql('sentiments', conn, if_exists='replace', index=False)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99905d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'cleaned_sentiment_dataset.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = 'cleaned_sentiment_dataset.csv'\n",
    "\n",
    "if os.path.isfile(file_path):\n",
    "    print(f\"The file '{file_path}' has been created successfully.\")\n",
    "else:\n",
    "    print(f\"The file '{file_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8d1eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'cleaned_sentiment_dataset.csv' is in the directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = '.'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "if file_path in files:\n",
    "    print(f\"The file '{file_path}' is in the directory.\")\n",
    "else:\n",
    "    print(f\"The file '{file_path}' is not found in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f4affbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'cleaned_sentiment_dataset.csv' has been read successfully.\n",
      "   Unnamed: 0.1  Unnamed: 0  \\\n",
      "0             0           0   \n",
      "1             1           1   \n",
      "2             2           2   \n",
      "3             3           3   \n",
      "4             4           4   \n",
      "\n",
      "                                                Text    Sentiment  \\\n",
      "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
      "1   Traffic was terrible this morning.           ...   Negative     \n",
      "2   Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
      "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
      "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
      "\n",
      "             Timestamp            User     Platform  \\\n",
      "0  2023-01-15 12:30:00   User123          Twitter     \n",
      "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
      "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
      "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
      "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
      "\n",
      "                                     Hashtags  Retweets  Likes       Country  \\\n",
      "0   #Nature #Park                                  15.0   30.0     USA         \n",
      "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
      "2   #Fitness #Workout                              20.0   40.0   USA           \n",
      "3   #Travel #Adventure                              8.0   15.0     UK          \n",
      "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
      "\n",
      "   Year  Month  Day  Hour                      cleaned_text  \n",
      "0  2023      1   15    12       enjoying beautiful day park  \n",
      "1  2023      1   15     8          traffic terrible morning  \n",
      "2  2023      1   15    15          finished amazing workout  \n",
      "3  2023      1   15    18  excited upcoming weekend getaway  \n",
      "4  2023      1   15    19  trying new recipe dinner tonight  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df_check = pd.read_csv(file_path)\n",
    "    print(f\"The file '{file_path}' has been read successfully.\")\n",
    "    print(df_check.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{file_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78472e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sentimentdataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b4578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
